# -*- coding: utf-8 -*-
"""01-ETL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-KjpW80cdgqELWYHJGmEwsPtrS1Nvhc1
"""

# Install for read .env file
!pip install python-dotenv

import os
from dotenv import load_dotenv

# read .env file into environment variable 
load_dotenv(override=True)

# use os.getenv() for read variable 
class Config:
  MYSQL_HOST = os.getenv("MYSQL_HOST")
  MYSQL_PORT = int(os.getenv("MYSQL_PORT"))
  MYSQL_USER = os.getenv("MYSQL_USER")
  MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD")
  MYSQL_DB = os.getenv("MYSQL_DB")
  MYSQL_CHARSET = os.getenv("MYSQL_CHARSET")

print(Config.MYSQL_HOST)

"""# Connect to DB"""

# Install PyMySQL
! pip install pymysql

import pymysql
def sql_exec(query):
  conn = pymysql.connect(host=Config.MYSQL_HOST,
                              port=Config.MYSQL_PORT,
                              user=Config.MYSQL_USER,
                              password=Config.MYSQL_PASSWORD,
                              db=Config       .MYSQL_DB,
                              charset=Config.MYSQL_CHARSET,
                              cursorclass=pymysql.cursors.DictCursor)
  curs = conn.cursor()
  curs.execute(query)
  result = curs.fetchall()
  curs.close()
  conn.close()
  return result

"""# Spark"""

# Download Java Virtual Machine (JVM)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Download Spark
## https://spark.apache.org/downloads.html
## Click the link for downloading Spark
## Copy the first link on the web page 
!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
# Unzip the file
!tar xf spark-3.3.1-bin-hadoop3.tgz

# Set up the environment for Spark.
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = '/content/spark-3.3.1-bin-hadoop3'

# Install library for finding Spark
!pip install -q findspark
# Import the libary
import findspark
# Initiate findspark
findspark.init()
# Check the location for Spark
findspark.find()

# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()
# Check Spark Session Information
spark

qr = 'select * from audible_data'
result = sql_exec(qr)
audible_data = spark.createDataFrame(result)
audible_data.show(2)

qr = 'select * from audible_transaction'
result = sql_exec(qr)
audible_tx = spark.createDataFrame(result)
audible_tx = audible_tx.withColumnRenamed('book_id','book_id_2')
audible_tx.show(2)

tx = audible_data.join(audible_tx, audible_data.Book_ID == audible_tx.book_id_2,"left")
tx = tx.drop(tx.book_id_2)
tx.printSchema()

"""# REST API"""

import requests
url = "https://r2de2-workshop-vmftiryt6q-ts.a.run.app/usd_thb_conversion_rate"
r = requests.get(url)
rate_json = r.json() #dict
rate_dict = rate_json['conversion_rate'] #dict
rate_list = [(k, v) for k, v in rate_dict.items()] # dictionary to list of tuples

col = ["date","conversion_rate"]
conversion_rate = spark.createDataFrame(rate_list,col)
conversion_rate.show(5)

"""# Transform"""

tx.show(1)
conversion_rate.show(1)

from pyspark.sql.functions import *
tx = tx.withColumn('to_date',to_date(tx.timestamp))
final_df = tx.join(conversion_rate, tx.to_date == conversion_rate.date, 'inner')
final_df = final_df.drop(final_df.to_date)
final_df.printSchema()

final_df = final_df.withColumn('Price',regexp_replace(final_df.Price,'\$',''))
final_df = final_df.withColumn('THBPrice',final_df.Price*final_df.Rating)
final_df.select(final_df.Price,final_df.THBPrice).show(2)
final_df.show(2)

"""# Save to CSV file
ปกติแล้ว Spark จะทำการ Save ออกมาเป็นหลายไฟล์ เพราะใช้หลายเครื่องในการประมวลผล


*   partitioned files
*   single file


"""

# partitioned files (ใช้ multiple workers)
final_df.write.format("csv").options(header='True', delimiter=',').mode('overwrite').save("ws1-output.csv")

# 1 file (ใช้ single worker)
final_df.coalesce(1).write.format("csv").options(header='True', delimiter=',').mode('overwrite').save("ws1-output-single.csv")

# TEST Read file
parts = '/content/ws1-output.csv/part-*.csv'
read_parts = spark.read.format("csv").options(header='True', delimiter=',').load(parts)
print(read_parts.count())

part = '/content/ws1-output-single.csv/part-*.csv'
read_part = spark.read.format("csv").options(header='True', delimiter=',').load(part)
print(read_part.count())

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/01_ETL.ipynb


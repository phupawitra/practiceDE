# -*- coding: utf-8 -*-
"""02-Cleansing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/152to828iOR2fVGFiEQEIeEgAUx68mu5U

# Prepare envieronment
"""

# Download Java Virtual Machine (JVM)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Download Spark
## https://spark.apache.org/downloads.html
## Click the link for downloading Spark
## Copy the first link on the web page 
!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
# Unzip the file
!tar xf spark-3.3.1-bin-hadoop3.tgz

# Set up the environment for Spark.
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = '/content/spark-3.3.1-bin-hadoop3'

# Install library for finding Spark
!pip install -q findspark
# Import the libary
import findspark
# Initiate findspark
findspark.init()
# Check the location for Spark
findspark.find()

# Import SparkSession
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()
# Check Spark Session Information
spark

# Check version (Python & Spark)
import sys
print("Python version:",sys.version_info)
print("Spark version:",spark.version)

!rm data.zip
!rm ws2_data.csv

# Download data file
!wget -O data.zip https://file.designil.com/zdOfUE+
!unzip data.zip

# Read data file to Spark
dt = spark.read.csv('/content/ws2_data.csv', header = True, inferSchema = True, )


## wget = download file
## wget -O = ตั้งชื่อไฟล์

from pyspark.sql import functions as f
dt = dt.withColumn("timestamp", f.from_unixtime(f.unix_timestamp(dt.timestamp), "yyyy-MM-dd HH:mm:ss"))
dt

"""# Data Profiling"""

print('-----  dataframe  -----')
print(dt)

print('\n-----  col_nm  -----')
print(dt.columns)

print('\n-----  data_type  -----')
print(dt.dtypes)

print('\n-----  Show schema  -----')
dt.printSchema()

print('\n-----  Show data  -----')
dt.show(1)

print('\n-----  Count row  -----')
print(dt.count())

print('\n-----  Count col  -----')
print(len(dt.columns))

print('\n-----  Statistics  -----')
dt.describe().show()
dt.summary().show()
dt.select("price").describe().show()
## NaN = Not a Number

print('\n-----  Missing Values  -----')
dt.summary("count").show()
dt.filter(dt.user_id.isNull()).show()

"""# EDA - Exploratory Data Analysis"""

## Non-Graphical EDA

# ข้อมูลที่เป็นตัวเลข
dt.where(dt.price >= 1).show(1)

# ข้อมูลที่เป็นตัวหนังสือ
dt.where(dt.country == 'Canada').show(1)

# การซื้อทั้งหมดที่เกิดขึ้นในเดือนเมษายน มีกี่แถว
dt.filter(dt.timestamp.startswith("2021-04")).count()

## Graphical EDA
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Transform Spark Dataframe to Pandas Dataframe
dt_pd = dt.toPandas()
dt_pd.head(1)

# Boxplot - แสดงการกระจายตัวของข้อมูลตัวเลข
sns.boxplot(x = dt_pd['book_id'])

# Histogram - แสดงการกระจายตัวของข้อมูลตัวเลข
sns.histplot(dt_pd['price'], bins=10) #bins = จำนวน bar

# Check ว่า book_id เพิ่มขึ้นตามราคาไหม -> Plot ดูความสัมพันธ์ระหว่าง book_id กับ price
sns.scatterplot(x=dt_pd.book_id,y=dt_pd.price)

import plotly.express as px
fig = px.scatter(dt_pd, 'book_id', 'price')
fig.show()

"""# Data Cleansing with Spark

## Convert DataType
"""

from pyspark.sql import functions as func

dt.printSchema()

# Convert string to datetime
dt_clean = dt.withColumn( "timestamp",
                          func.to_timestamp(dt.timestamp, 'yyyy-MM-dd HH:mm:ss')
                        )
dt_clean.printSchema()

# นับยอด transaction ช่วงครึ่งเดือนแรก ของเดือนมิถุนายน
dt_clean.filter( (func.dayofmonth(dt_clean.timestamp) <= 15) & ( func.month(dt_clean.timestamp) == 6 ) ).count()

"""## Anomalies Check

1.   หาที่ผิด
2.   เพิ่ม col. ใหม่ เก็บค่าที่ถูกต้อง
3.   นำ col. ใหม่ แทนที่ col.เดิม

#### 1) Syntactical Anomalies
Lexical errors เช่น สะกดผิด
"""

# ใน Data set ชุดนี้ มีข้อมูลจากกี่ประเทศ
country_count = dt_clean.select(dt_clean.country).distinct().count()
print(country_count)

dt_clean.select(dt_clean.country).distinct().sort(dt_clean.country).show(country_count, False)


## sort = ทำให้ข้อมูลเรียงตามตัวอักษร อ่านง่ายขึ้น
## show() default 20 ตัว, ใส่ False เพื่อให้แสดงข้อมูลในคอลัมน์แบบเต็ม ๆ (หากไม่ใส่ คอลัมน์ที่ยาวจะถูกตัดตัวหนังสือ)

from pyspark.sql.functions import when

dt_clean_country = dt_clean.withColumn("countryUpdate", when(dt_clean.country == 'Japane', 'Japan').otherwise(dt_clean.country))
dt_clean_country_count = dt_clean_country.select(dt_clean_country.countryUpdate).distinct().count()
print(dt_clean_country_count)
dt_clean_country.select(dt_clean_country.countryUpdate).distinct().sort(dt_clean_country.countryUpdate).show(dt_clean_country_count, False)

# เอาคอลัมน์ CountryUpdate ไปแทนที่คอลัมน์ Country
dt_clean_country.show(1)
dt_clean = dt_clean_country.drop("Country").withColumnRenamed('countryUpdate', 'Country')
dt_clean.show(1)

"""#### 2) Semantic Anomalies

**Integrity constraints**: ค่าอยู่นอกเหนือขอบเขตของค่าที่รับได้ เช่น
- user_id: ค่าจะต้องเป็นตัวเลขหรือตัวหนังสือ 8 ตัวอักษร

"""

# หา format ที่ถูกต้อง ด้วย Regular Expression
dt_clean.select("user_id").show(5)
print(dt_clean.select("user_id").count())
print(dt_clean.filter(dt_clean.user_id.rlike("^[a-z0-9]{8}$")).count())

# หา format ที่ไม่ถูกต้อง
dt_correct_userid = dt_clean.filter(dt_clean.user_id.rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean.subtract(dt_correct_userid)
dt_incorrect_userid.show(1)


# แทนค่าที่ผิด ด้วยค่าที่ถูกต้อง
dt_clean_userid = dt_clean.withColumn("user_id_update",
                                        when(dt_clean.user_id == 'ca86d17200', 'ca86d172')
                                        .otherwise(dt_clean.user_id))

# เอาคอลัมน์ user_id_update ไปแทนที่ user_id
dt_clean_userid.show(5)
dt_clean = dt_clean_userid.drop("user_id").withColumnRenamed('user_id_update', 'user_id')
dt_clean.show(5)

"""#### 3) Missing values

ปล. เจอ null อย่าเพิ่งรีบลบ หาสาเหตุก่อน

ขั้นตอน
1. หา Row ที่ว่าง
2. เพิ่ม col. ใหม่ เก็บค่าที่ถูกต้อง
3. นำ col. ใหม่ แทนที่ col.เดิม
"""

# ทางทีม Data Analyst แจ้งว่าอยากให้เราแทน user_id ที่เป็น NULL ด้วย 00000000 

# เช็ค Missing Value
dt_clean.summary("count").show()
dt_clean.filter(dt_clean.user_id.isNull()).show()

# แทนค่าที่ว่าง ด้วย 00000000
dt_clean_user_id = dt_clean.withColumn("user_id_update",
                                        when(dt_clean.user_id.isNull(), '00000000')
                                        .otherwise(dt_clean.user_id))

# เอาคอลัมน์ user_id_update ไปแทนที่ user_id
dt_clean_userid.show(1)
dt_clean = dt_clean_user_id.drop("user_id").withColumnRenamed('user_id_update', 'user_id')
dt_clean.show(1)

dt_clean.filter( dt_clean.user_id.isNull() ).show()

"""#### 4) Outliers

Using Boxplot



1.   หาค่าที่ Outliers
2.   Check data ที่ Outliers -> ว่าที่ Outliers นั้นผิดปกติไหม


"""

# หาค่าที่ Outliers
dt_clean_pd = dt_clean.toPandas()
sns.boxplot(x = dt_clean_pd.price)

# Check data ที่ Outliers (Outliers > 80)
dt_clean.filter( dt_clean.price > 80 ).select("book_id").distinct().show()

## นำ Book_ID อันนี้ไปเช็คต่อกับแหล่งข้อมูลได้ ว่าเป็นหนังสืออะไร และราคาเกิน $80 ผิดปกติมั้ย
## The Power Broker มีราคา $84 จริง และเป็นหนังสือเสียงที่มีความยาวถึง 66 ชั่วโมง
## ในที่นี้ ถือว่าเป็น Outlier จริง แต่ไม่ได้เป็นข้อมูลที่ผิด จึงไม่ต้องแก้อะไร

"""# Data Cleansing with Spark SQL


1.   Transform DF to TempView
2.   ใช้ SQL ดึง หรือแปลง data บน TempView 
"""

# Transform Spark DataFrame to TempView
dt.createOrReplaceTempView("tbl_data")
dt_sql = spark.sql("SELECT * FROM tbl_data")
dt_sql.show(3)

# SQL -> List ชื่อประเทศ
dt_sql_country = spark.sql("""
SELECT distinct country
FROM tbl_data
ORDER BY country
""")
dt_sql_country.show(1)

# SQL -> Replace ชื่อประเทศ
dt_sql_result = spark.sql("""
SELECT timestamp, user_id, book_id,
  CASE
    WHEN country = 'Japane' THEN 'Japan' ELSE country 
  END AS country,
price
FROM tbl_data
""")
dt_sql_result.show(1,False)

# Check results
dt_sql_result.select("country").distinct().sort("country").show(58, False)

# Check user_id ที่ผิด format (ไม่เป็นตัวหนังสือ หรือตัวเลข 8 หลัก)
dt_sql_result = spark.sql("""
SELECT *
FROM tbl_data
WHERE user_id NOT RLIKE '^[a-z0-9]{8}$'
""")
dt_sql_result.show()


# Replace ค่าที่ผิด
dt_sql_result = spark.sql("""
SELECT timestamp, 
  CASE 
    WHEN user_id = 'ca86d17200' THEN 'ca86d172' ELSE user_id
  END AS user_id,
book_id,
country,
price
FROM tbl_data
""")
dt_sql_result.show()

# Check results
dt_sql_result.where( dt_sql_result.user_id == 'ca86d17200' ).show()

"""#  Save data เป็น CSV
ปกติแล้ว Spark จะทำการ Save ออกมาเป็นหลายไฟล์ เพราะใช้หลายเครื่องในการประมวลผล


*   partitioned files
*   single file
"""

# partitioned files (ใช้ multiple workers)
dt_clean.write.format("csv").options(header='True', delimiter=',').mode('overwrite').save("ws2_Cleaned_data.csv")

# 1 file (ใช้ single worker)
dt_clean.coalesce(1).write.format("csv").options(header='True', delimiter=',').mode('overwrite').save("ws2_Cleaned_Data_Single.csv")

!jupyter nbconvert --to html /content/02_Cleansing.ipynb

